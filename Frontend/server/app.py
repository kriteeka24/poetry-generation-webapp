from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import time

# Optional imports for real model generation (install via requirements.txt)
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    TRANSFORMERS_AVAILABLE = True
except Exception:
    TRANSFORMERS_AVAILABLE = False

app = Flask(__name__)
CORS(app)

PORT = int(os.environ.get("PORT", 5000))

# Development convenience: set FAKE_MODE=1 to return deterministic fake text
FAKE_MODE = os.environ.get("FAKE_MODE", "1") == "1"

# If you want to use local GPT-2 via transformers, set MODEL_NAME env var (e.g., "gpt2")
HF_GPT2_MODEL = os.environ.get("HF_GPT2_MODEL", "gpt2")

# Lazy-loaded model/tokenizer placeholders
_gpt2_tokenizer = None
_gpt2_model = None

def load_gpt2():
    global _gpt2_tokenizer, _gpt2_model
    if _gpt2_tokenizer is None or _gpt2_model is None:
        if not TRANSFORMERS_AVAILABLE:
            raise RuntimeError("transformers or torch not installed. Install optional deps to enable GPT-2 generation.")
        _gpt2_tokenizer = AutoTokenizer.from_pretrained(HF_GPT2_MODEL)
        _gpt2_model = AutoModelForCausalLM.from_pretrained(HF_GPT2_MODEL)
        # Put model on GPU if available
        if torch.cuda.is_available():
            _gpt2_model = _gpt2_model.to("cuda")
    return _gpt2_tokenizer, _gpt2_model

@app.route("/api/generate", methods=["POST"])
def generate():
    data = request.get_json() or {}
    model = data.get("model")
    prompt = data.get("prompt")
    max_length = int(data.get("max_length", 120))
    temperature = float(data.get("temperature", 0.8))
    top_k = int(data.get("top_k", 50))
    top_p = float(data.get("top_p", 0.9))
    repetition_penalty = float(data.get("repetition_penalty", 1.0))
    num_beams = int(data.get("num_beams", 1))
    do_sample = bool(data.get("do_sample", True))

    if not model or not prompt:
        return jsonify({"error": "Missing 'model' or 'prompt' in request body"}), 400

    # Development mode: return deterministic fake poem
    if FAKE_MODE:
        fake = (
            f"{prompt}\n\n"
            f"(Generated by {model.upper()})\n"
            f"Parameters: len={max_length}, temp={temperature:.2f}, top_k={top_k}, "
            f"top_p={top_p:.2f}, rep_penalty={repetition_penalty:.2f}, beams={num_beams}\n\n"
            "Softly the sunrise slides over fields of code,\n"
            "Lines like rivers, memory in ode.\n"
            "A gentle rhyme of bytes and dreams,\n"
            "Whispers of logic in silver beams.\n\n"
            "Through digital realms where thoughts take flight,\n"
            "The neural pathways dance in light.\n"
            "Each token born from probability's embrace,\n"
            "Creating verses with algorithmic grace."
        )
        # slight delay to simulate processing
        time.sleep(0.8)
        return jsonify({"generated_text": fake})

    # If not FAKE_MODE, try to generate for GPT-2 using transformers (local)
    if model == "gpt2":
        try:
            tokenizer, gpt2 = load_gpt2()
            input_ids = tokenizer.encode(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                input_ids = input_ids.to("cuda")
            # cap max_length to avoid runaway
            max_total_length = min(1024, len(input_ids[0]) + max_length)
            
            # Build generation kwargs based on parameters
            gen_kwargs = {
                "max_length": max_total_length,
                "num_return_sequences": 1,
                "pad_token_id": tokenizer.eos_token_id,
                "repetition_penalty": max(1.0, min(2.0, repetition_penalty)),
            }
            
            if do_sample:
                gen_kwargs.update({
                    "do_sample": True,
                    "temperature": max(0.1, min(2.0, temperature)),
                    "top_k": max(1, min(100, top_k)),
                    "top_p": max(0.1, min(1.0, top_p)),
                })
            else:
                gen_kwargs["do_sample"] = False
                if num_beams > 1:
                    gen_kwargs["num_beams"] = min(5, num_beams)
            
            outputs = gpt2.generate(input_ids, **gen_kwargs)
            text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            return jsonify({"generated_text": text})
        except Exception as e:
            return jsonify({"error": f"GPT-2 generation failed: {str(e)}"}), 500

    # LSTM route: integrate your trained LSTM here.
    if model == "lstm":
        # Example placeholder: you should replace this with a function that
        # loads your LSTM weights and tokenizer and returns generated text.
        # Below is a simple placeholder message to guide integration.
        placeholder = (
            f"{prompt}\n\n"
            f"(LSTM generation)\n"
            f"Parameters: len={max_length}, temp={temperature:.2f}, top_k={top_k}\n\n"
            "Note: LSTM model uses temperature and top_k for sampling.\n"
            "Other parameters (top_p, beams) are specific to Transformers.\n\n"
            "Please integrate your trained LSTM model in server/app.py."
        )
        return jsonify({"generated_text": placeholder})

    return jsonify({"error": f"Unsupported model: {model}"}), 400

if __name__ == "__main__":
    print(f"Starting Flask server on port {PORT} (FAKE_MODE={FAKE_MODE})")
    app.run(host="0.0.0.0", port=PORT)