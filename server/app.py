from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import time

# Optional imports for real model generation (install via requirements.txt)
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    TRANSFORMERS_AVAILABLE = True
except Exception:
    TRANSFORMERS_AVAILABLE = False

app = Flask(__name__)
CORS(app)

PORT = int(os.environ.get("PORT", 5000))

# Development convenience: set FAKE_MODE=1 to return deterministic fake text
FAKE_MODE = os.environ.get("FAKE_MODE", "1") == "1"

# If you want to use local GPT-2 via transformers, set MODEL_NAME env var (e.g., "gpt2")
HF_GPT2_MODEL = os.environ.get("HF_GPT2_MODEL", "gpt2")

# Lazy-loaded model/tokenizer placeholders
_gpt2_tokenizer = None
_gpt2_model = None

def load_gpt2():
    global _gpt2_tokenizer, _gpt2_model
    if _gpt2_tokenizer is None or _gpt2_model is None:
        if not TRANSFORMERS_AVAILABLE:
            raise RuntimeError("transformers or torch not installed. Install optional deps to enable GPT-2 generation.")
        _gpt2_tokenizer = AutoTokenizer.from_pretrained(HF_GPT2_MODEL)
        _gpt2_model = AutoModelForCausalLM.from_pretrained(HF_GPT2_MODEL)
        # Put model on GPU if available
        if torch.cuda.is_available():
            _gpt2_model = _gpt2_model.to("cuda")
    return _gpt2_tokenizer, _gpt2_model

@app.route("/api/generate", methods=["POST"])
def generate():
    data = request.get_json() or {}
    model = data.get("model")
    prompt = data.get("prompt")
    max_length = int(data.get("max_length", 120))
    temperature = float(data.get("temperature", 0.8))

    if not model or not prompt:
        return jsonify({"error": "Missing 'model' or 'prompt' in request body"}), 400

    # Development mode: return deterministic fake poem
    if FAKE_MODE:
        fake = (
            f"{prompt}\n\n"
            f"(Generated by {model.upper()} — len={max_length}, temp={temperature})\n\n"
            "Softly the sunrise slides over fields of code,\n"
            "Lines like rivers, memory in ode.\n"
            "A gentle rhyme of bytes and dreams,\n"
            "Whispers of logic in silver beams."
        )
        # slight delay to simulate processing
        time.sleep(0.8)
        return jsonify({"generated_text": fake})

    # If not FAKE_MODE, try to generate for GPT-2 using transformers (local)
    if model == "gpt2":
        try:
            tokenizer, gpt2 = load_gpt2()
            input_ids = tokenizer.encode(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                input_ids = input_ids.to("cuda")
            # cap max_length to avoid runaway
            max_total_length = min(1024, len(input_ids[0]) + max_length)
            outputs = gpt2.generate(
                input_ids,
                do_sample=True,
                max_length=max_total_length,
                temperature=max(0.1, min(2.0, temperature)),
                top_p=0.9,
                num_return_sequences=1,
                pad_token_id=tokenizer.eos_token_id
            )
            text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            return jsonify({"generated_text": text})
        except Exception as e:
            return jsonify({"error": f"GPT-2 generation failed: {str(e)}"}), 500

    # LSTM route: integrate your trained LSTM here.
    if model == "lstm":
        # Example placeholder: you should replace this with a function that
        # loads your LSTM weights and tokenizer and returns generated text.
        # Below is a simple placeholder message to guide integration.
        placeholder = (
            f"{prompt}\n\n"
            f"(LSTM generation placeholder — len={max_length}, temp={temperature})\n\n"
            "Please replace the LSTM placeholder in server/app.py with your code that\n"
            "loads your trained LSTM model and returns the generated sequence."
        )
        return jsonify({"generated_text": placeholder})

    return jsonify({"error": f"Unsupported model: {model}"}), 400

if __name__ == "__main__":
    print(f"Starting Flask server on port {PORT} (FAKE_MODE={FAKE_MODE})")
    app.run(host="0.0.0.0", port=PORT)